{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "from time import time\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn import (linear_model, decomposition, datasets,manifold, datasets, decomposition, \n",
    "                     ensemble, discriminant_analysis, random_projection, neighbors,metrics)\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import GridSearchCV,train_test_split\n",
    "from sklearn.preprocessing import scale, StandardScaler\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.linear_model import BayesianRidge, LinearRegression\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.feature_selection import RFE\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.feature_selection import VarianceThreshold, SelectKBest, chi2, mutual_info_classif\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.utils import resample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "#reading Excel file from PANDAS\n",
    "filename = \"data.xlsx\"\n",
    "Data = pd.read_excel(filename,header=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "#separating atributes from class\n",
    "atributes=Data.drop('Class', axis=1)\n",
    "classes=Data[['Class']]\n",
    "X=Data.drop('Class', axis=1).values\n",
    "Y=np.squeeze(Data[['Class']].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_new = SelectKBest(mutual_info_classif, k=17).fit_transform(X, Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler=StandardScaler() #choosing scaler\n",
    "scaler.fit(X_new) # compute the mean and standard which will be used in the next command\n",
    "X_scaled=scaler.transform(X_new)# fit and transform can be applied together and I leave that for simple exercise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Tuning hyper-parameters for accuracy\n",
      "\n",
      "Best parameters set found on development set:\n",
      "\n",
      "{'C': 10000, 'gamma': 0.001, 'kernel': 'rbf'}\n",
      "\n",
      "Grid scores on development set:\n",
      "\n",
      "0.945 (+/-0.013) for {'C': 1, 'gamma': 0.001, 'kernel': 'rbf'}\n",
      "0.714 (+/-0.014) for {'C': 1, 'gamma': 0.0001, 'kernel': 'rbf'}\n",
      "0.951 (+/-0.015) for {'C': 10, 'gamma': 0.001, 'kernel': 'rbf'}\n",
      "0.945 (+/-0.012) for {'C': 10, 'gamma': 0.0001, 'kernel': 'rbf'}\n",
      "0.953 (+/-0.016) for {'C': 100, 'gamma': 0.001, 'kernel': 'rbf'}\n",
      "0.951 (+/-0.015) for {'C': 100, 'gamma': 0.0001, 'kernel': 'rbf'}\n",
      "0.953 (+/-0.016) for {'C': 1000, 'gamma': 0.001, 'kernel': 'rbf'}\n",
      "0.953 (+/-0.015) for {'C': 1000, 'gamma': 0.0001, 'kernel': 'rbf'}\n",
      "0.955 (+/-0.016) for {'C': 10000, 'gamma': 0.001, 'kernel': 'rbf'}\n",
      "0.954 (+/-0.016) for {'C': 10000, 'gamma': 0.0001, 'kernel': 'rbf'}\n",
      "\n",
      "Detailed classification report:\n",
      "\n",
      "The model is trained on the full development set.\n",
      "The scores are computed on the full evaluation set.\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.95      0.99      0.97       599\n",
      "           2       1.00      0.04      0.08        25\n",
      "           3       0.98      0.98      0.98       162\n",
      "           4       1.00      1.00      1.00        10\n",
      "           5       0.99      0.97      0.98        72\n",
      "           6       1.00      1.00      1.00         7\n",
      "           7       1.00      0.95      0.97        19\n",
      "           8       1.00      1.00      1.00        77\n",
      "\n",
      "    accuracy                           0.96       971\n",
      "   macro avg       0.99      0.87      0.87       971\n",
      "weighted avg       0.97      0.96      0.95       971\n",
      "\n",
      "Confusion Matrix\n",
      "[[594   0   4   0   1   0   0   0]\n",
      " [ 24   1   0   0   0   0   0   0]\n",
      " [  3   0 159   0   0   0   0   0]\n",
      " [  0   0   0  10   0   0   0   0]\n",
      " [  2   0   0   0  70   0   0   0]\n",
      " [  0   0   0   0   0   7   0   0]\n",
      " [  1   0   0   0   0   0  18   0]\n",
      " [  0   0   0   0   0   0   0  77]]\n",
      "F1 Score\n",
      "0.9529017199082452\n",
      "Accuracy Score\n",
      "0.9639546858908342\n",
      "\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, Y_train, Y_test = train_test_split(X_scaled, Y, train_size=0.05,test_size=0.01, random_state=0)\n",
    "\n",
    "# Set the parameters by cross-validation\n",
    "tuned_parameters = [{'kernel': ['rbf'], 'gamma': [1e-3, 1e-4],'C': [1, 10, 100, 1000, 10000]}]\n",
    "\n",
    "scores = ['accuracy']\n",
    "\n",
    "for score in scores:\n",
    "    print(\"# Tuning hyper-parameters for %s\" % score)\n",
    "    print()\n",
    "\n",
    "    clf = GridSearchCV(SVC(), tuned_parameters, cv=10,\n",
    "                       scoring=score)\n",
    "    clf.fit(X_train, Y_train)\n",
    "\n",
    "    print(\"Best parameters set found on development set:\")\n",
    "    print()\n",
    "    print(clf.best_params_)\n",
    "    print()\n",
    "    print(\"Grid scores on development set:\")\n",
    "    print()\n",
    "    means = clf.cv_results_['mean_test_score']\n",
    "    stds = clf.cv_results_['std_test_score']\n",
    "    for mean, std, params in zip(means, stds, clf.cv_results_['params']):\n",
    "        print(\"%0.3f (+/-%0.03f) for %r\"\n",
    "              % (mean, std * 2, params))\n",
    "    print()\n",
    "\n",
    "    print(\"Detailed classification report:\")\n",
    "    print()\n",
    "    print(\"The model is trained on the full development set.\")\n",
    "    print(\"The scores are computed on the full evaluation set.\")\n",
    "    print()\n",
    "    Y_true, Y_pred = Y_test, clf.predict(X_test)\n",
    "    print(metrics.classification_report(Y_true, Y_pred))\n",
    "    print(\"Confusion Matrix\")\n",
    "    print(metrics.confusion_matrix(Y_true, Y_pred))\n",
    "    print(\"F1 Score\")\n",
    "    print(metrics.f1_score(Y_test, Y_pred, average='weighted'))\n",
    "    print(\"Accuracy Score\")\n",
    "    print(metrics.accuracy_score(Y_true, Y_pred, normalize=True))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Tuning hyper-parameters for accuracy\n",
      "\n",
      "Best parameters set found on development set:\n",
      "\n",
      "{'activation': 'relu', 'alpha': 0.001, 'hidden_layer_sizes': (50,)}\n",
      "\n",
      "Grid scores on development set:\n",
      "\n",
      "0.954 (+/-0.017) for {'activation': 'tanh', 'alpha': 0.01, 'hidden_layer_sizes': (20,)}\n",
      "0.955 (+/-0.017) for {'activation': 'tanh', 'alpha': 0.01, 'hidden_layer_sizes': (40,)}\n",
      "0.955 (+/-0.017) for {'activation': 'tanh', 'alpha': 0.01, 'hidden_layer_sizes': (50,)}\n",
      "0.954 (+/-0.017) for {'activation': 'tanh', 'alpha': 0.01, 'hidden_layer_sizes': (100,)}\n",
      "0.954 (+/-0.017) for {'activation': 'tanh', 'alpha': 0.001, 'hidden_layer_sizes': (20,)}\n",
      "0.954 (+/-0.017) for {'activation': 'tanh', 'alpha': 0.001, 'hidden_layer_sizes': (40,)}\n",
      "0.955 (+/-0.017) for {'activation': 'tanh', 'alpha': 0.001, 'hidden_layer_sizes': (50,)}\n",
      "0.954 (+/-0.017) for {'activation': 'tanh', 'alpha': 0.001, 'hidden_layer_sizes': (100,)}\n",
      "0.954 (+/-0.017) for {'activation': 'tanh', 'alpha': 0.0001, 'hidden_layer_sizes': (20,)}\n",
      "0.954 (+/-0.017) for {'activation': 'tanh', 'alpha': 0.0001, 'hidden_layer_sizes': (40,)}\n",
      "0.955 (+/-0.017) for {'activation': 'tanh', 'alpha': 0.0001, 'hidden_layer_sizes': (50,)}\n",
      "0.955 (+/-0.016) for {'activation': 'tanh', 'alpha': 0.0001, 'hidden_layer_sizes': (100,)}\n",
      "0.955 (+/-0.017) for {'activation': 'relu', 'alpha': 0.01, 'hidden_layer_sizes': (20,)}\n",
      "0.955 (+/-0.017) for {'activation': 'relu', 'alpha': 0.01, 'hidden_layer_sizes': (40,)}\n",
      "0.955 (+/-0.017) for {'activation': 'relu', 'alpha': 0.01, 'hidden_layer_sizes': (50,)}\n",
      "0.955 (+/-0.017) for {'activation': 'relu', 'alpha': 0.01, 'hidden_layer_sizes': (100,)}\n",
      "0.955 (+/-0.017) for {'activation': 'relu', 'alpha': 0.001, 'hidden_layer_sizes': (20,)}\n",
      "0.955 (+/-0.017) for {'activation': 'relu', 'alpha': 0.001, 'hidden_layer_sizes': (40,)}\n",
      "0.956 (+/-0.017) for {'activation': 'relu', 'alpha': 0.001, 'hidden_layer_sizes': (50,)}\n",
      "0.955 (+/-0.017) for {'activation': 'relu', 'alpha': 0.001, 'hidden_layer_sizes': (100,)}\n",
      "0.955 (+/-0.017) for {'activation': 'relu', 'alpha': 0.0001, 'hidden_layer_sizes': (20,)}\n",
      "0.955 (+/-0.017) for {'activation': 'relu', 'alpha': 0.0001, 'hidden_layer_sizes': (40,)}\n",
      "0.956 (+/-0.017) for {'activation': 'relu', 'alpha': 0.0001, 'hidden_layer_sizes': (50,)}\n",
      "0.955 (+/-0.016) for {'activation': 'relu', 'alpha': 0.0001, 'hidden_layer_sizes': (100,)}\n",
      "\n",
      "Detailed classification report:\n",
      "\n",
      "The model is trained on the full development set.\n",
      "The scores are computed on the full evaluation set.\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.95      0.99      0.97       599\n",
      "           2       1.00      0.04      0.08        25\n",
      "           3       0.98      0.98      0.98       162\n",
      "           4       1.00      1.00      1.00        10\n",
      "           5       0.99      0.97      0.98        72\n",
      "           6       1.00      1.00      1.00         7\n",
      "           7       1.00      0.95      0.97        19\n",
      "           8       1.00      1.00      1.00        77\n",
      "\n",
      "    accuracy                           0.96       971\n",
      "   macro avg       0.99      0.87      0.87       971\n",
      "weighted avg       0.97      0.96      0.95       971\n",
      "\n",
      "Confusion Matrix\n",
      "[[594   0   4   0   1   0   0   0]\n",
      " [ 24   1   0   0   0   0   0   0]\n",
      " [  3   0 159   0   0   0   0   0]\n",
      " [  0   0   0  10   0   0   0   0]\n",
      " [  2   0   0   0  70   0   0   0]\n",
      " [  0   0   0   0   0   7   0   0]\n",
      " [  1   0   0   0   0   0  18   0]\n",
      " [  0   0   0   0   0   0   0  77]]\n",
      "F1 Score\n",
      "0.9529017199082452\n",
      "Accuracy Score\n",
      "0.9639546858908342\n",
      "\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, Y_train, Y_test = train_test_split(X_scaled, Y, train_size=0.05,test_size=0.01, random_state=0)\n",
    "\n",
    "# Set the parameters by cross-validation\n",
    "tuned_parameters = [{'activation': ['tanh', 'relu'], 'alpha': [1e-2, 1e-3, 1e-4]\n",
    "                     ,'hidden_layer_sizes':[(20,),(40,),(50,),(100,)]}]\n",
    "\n",
    "scores = ['accuracy']\n",
    "\n",
    "for score in scores:\n",
    "    print(\"# Tuning hyper-parameters for %s\" % score)\n",
    "    print()\n",
    "\n",
    "    clf = GridSearchCV(MLPClassifier(max_iter=500,solver='sgd', verbose=False, tol=1e-4, random_state=1,learning_rate_init=0.075), tuned_parameters, cv=10,\n",
    "                       scoring=score)\n",
    "    clf.fit(X_train, Y_train)\n",
    "\n",
    "    print(\"Best parameters set found on development set:\")\n",
    "    print()\n",
    "    print(clf.best_params_)\n",
    "    print()\n",
    "    print(\"Grid scores on development set:\")\n",
    "    print()\n",
    "    means = clf.cv_results_['mean_test_score']\n",
    "    stds = clf.cv_results_['std_test_score']\n",
    "    for mean, std, params in zip(means, stds, clf.cv_results_['params']):\n",
    "        print(\"%0.3f (+/-%0.03f) for %r\"\n",
    "              % (mean, std * 2, params))\n",
    "    print()\n",
    "\n",
    "    print(\"Detailed classification report:\")\n",
    "    print()\n",
    "    print(\"The model is trained on the full development set.\")\n",
    "    print(\"The scores are computed on the full evaluation set.\")\n",
    "    print()\n",
    "    Y_true, Y_pred = Y_test, clf.predict(X_test)\n",
    "    print(metrics.classification_report(Y_true, Y_pred))\n",
    "    print(\"Confusion Matrix\")\n",
    "    print(metrics.confusion_matrix(Y_true, Y_pred))\n",
    "    print(\"F1 Score\")\n",
    "    print(metrics.f1_score(Y_test, Y_pred, average='weighted'))\n",
    "    print(\"Accuracy Score\")\n",
    "    print(metrics.accuracy_score(Y_true, Y_pred, normalize=True))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Tuning hyper-parameters for accuracy\n",
      "\n",
      "Best parameters set found on development set:\n",
      "\n",
      "{'leaf_size': 20, 'n_neighbors': 5, 'p': 2, 'weights': 'uniform'}\n",
      "\n",
      "Grid scores on development set:\n",
      "\n",
      "0.952 (+/-0.016) for {'leaf_size': 20, 'n_neighbors': 5, 'p': 1, 'weights': 'uniform'}\n",
      "0.941 (+/-0.019) for {'leaf_size': 20, 'n_neighbors': 5, 'p': 1, 'weights': 'distance'}\n",
      "0.952 (+/-0.017) for {'leaf_size': 20, 'n_neighbors': 5, 'p': 2, 'weights': 'uniform'}\n",
      "0.941 (+/-0.018) for {'leaf_size': 20, 'n_neighbors': 5, 'p': 2, 'weights': 'distance'}\n",
      "0.947 (+/-0.013) for {'leaf_size': 20, 'n_neighbors': 10, 'p': 1, 'weights': 'uniform'}\n",
      "0.942 (+/-0.017) for {'leaf_size': 20, 'n_neighbors': 10, 'p': 1, 'weights': 'distance'}\n",
      "0.949 (+/-0.014) for {'leaf_size': 20, 'n_neighbors': 10, 'p': 2, 'weights': 'uniform'}\n",
      "0.943 (+/-0.017) for {'leaf_size': 20, 'n_neighbors': 10, 'p': 2, 'weights': 'distance'}\n",
      "0.946 (+/-0.011) for {'leaf_size': 20, 'n_neighbors': 15, 'p': 1, 'weights': 'uniform'}\n",
      "0.942 (+/-0.018) for {'leaf_size': 20, 'n_neighbors': 15, 'p': 1, 'weights': 'distance'}\n",
      "0.946 (+/-0.012) for {'leaf_size': 20, 'n_neighbors': 15, 'p': 2, 'weights': 'uniform'}\n",
      "0.943 (+/-0.017) for {'leaf_size': 20, 'n_neighbors': 15, 'p': 2, 'weights': 'distance'}\n",
      "0.945 (+/-0.011) for {'leaf_size': 20, 'n_neighbors': 20, 'p': 1, 'weights': 'uniform'}\n",
      "0.940 (+/-0.013) for {'leaf_size': 20, 'n_neighbors': 20, 'p': 1, 'weights': 'distance'}\n",
      "0.946 (+/-0.012) for {'leaf_size': 20, 'n_neighbors': 20, 'p': 2, 'weights': 'uniform'}\n",
      "0.941 (+/-0.016) for {'leaf_size': 20, 'n_neighbors': 20, 'p': 2, 'weights': 'distance'}\n",
      "0.952 (+/-0.016) for {'leaf_size': 30, 'n_neighbors': 5, 'p': 1, 'weights': 'uniform'}\n",
      "0.941 (+/-0.019) for {'leaf_size': 30, 'n_neighbors': 5, 'p': 1, 'weights': 'distance'}\n",
      "0.952 (+/-0.017) for {'leaf_size': 30, 'n_neighbors': 5, 'p': 2, 'weights': 'uniform'}\n",
      "0.941 (+/-0.018) for {'leaf_size': 30, 'n_neighbors': 5, 'p': 2, 'weights': 'distance'}\n",
      "0.947 (+/-0.013) for {'leaf_size': 30, 'n_neighbors': 10, 'p': 1, 'weights': 'uniform'}\n",
      "0.942 (+/-0.017) for {'leaf_size': 30, 'n_neighbors': 10, 'p': 1, 'weights': 'distance'}\n",
      "0.949 (+/-0.014) for {'leaf_size': 30, 'n_neighbors': 10, 'p': 2, 'weights': 'uniform'}\n",
      "0.943 (+/-0.017) for {'leaf_size': 30, 'n_neighbors': 10, 'p': 2, 'weights': 'distance'}\n",
      "0.946 (+/-0.011) for {'leaf_size': 30, 'n_neighbors': 15, 'p': 1, 'weights': 'uniform'}\n",
      "0.942 (+/-0.018) for {'leaf_size': 30, 'n_neighbors': 15, 'p': 1, 'weights': 'distance'}\n",
      "0.946 (+/-0.012) for {'leaf_size': 30, 'n_neighbors': 15, 'p': 2, 'weights': 'uniform'}\n",
      "0.943 (+/-0.017) for {'leaf_size': 30, 'n_neighbors': 15, 'p': 2, 'weights': 'distance'}\n",
      "0.945 (+/-0.011) for {'leaf_size': 30, 'n_neighbors': 20, 'p': 1, 'weights': 'uniform'}\n",
      "0.940 (+/-0.013) for {'leaf_size': 30, 'n_neighbors': 20, 'p': 1, 'weights': 'distance'}\n",
      "0.946 (+/-0.012) for {'leaf_size': 30, 'n_neighbors': 20, 'p': 2, 'weights': 'uniform'}\n",
      "0.941 (+/-0.016) for {'leaf_size': 30, 'n_neighbors': 20, 'p': 2, 'weights': 'distance'}\n",
      "0.952 (+/-0.016) for {'leaf_size': 50, 'n_neighbors': 5, 'p': 1, 'weights': 'uniform'}\n",
      "0.941 (+/-0.019) for {'leaf_size': 50, 'n_neighbors': 5, 'p': 1, 'weights': 'distance'}\n",
      "0.952 (+/-0.017) for {'leaf_size': 50, 'n_neighbors': 5, 'p': 2, 'weights': 'uniform'}\n",
      "0.941 (+/-0.018) for {'leaf_size': 50, 'n_neighbors': 5, 'p': 2, 'weights': 'distance'}\n",
      "0.947 (+/-0.013) for {'leaf_size': 50, 'n_neighbors': 10, 'p': 1, 'weights': 'uniform'}\n",
      "0.942 (+/-0.017) for {'leaf_size': 50, 'n_neighbors': 10, 'p': 1, 'weights': 'distance'}\n",
      "0.949 (+/-0.014) for {'leaf_size': 50, 'n_neighbors': 10, 'p': 2, 'weights': 'uniform'}\n",
      "0.943 (+/-0.017) for {'leaf_size': 50, 'n_neighbors': 10, 'p': 2, 'weights': 'distance'}\n",
      "0.946 (+/-0.011) for {'leaf_size': 50, 'n_neighbors': 15, 'p': 1, 'weights': 'uniform'}\n",
      "0.942 (+/-0.018) for {'leaf_size': 50, 'n_neighbors': 15, 'p': 1, 'weights': 'distance'}\n",
      "0.946 (+/-0.012) for {'leaf_size': 50, 'n_neighbors': 15, 'p': 2, 'weights': 'uniform'}\n",
      "0.943 (+/-0.017) for {'leaf_size': 50, 'n_neighbors': 15, 'p': 2, 'weights': 'distance'}\n",
      "0.945 (+/-0.011) for {'leaf_size': 50, 'n_neighbors': 20, 'p': 1, 'weights': 'uniform'}\n",
      "0.940 (+/-0.013) for {'leaf_size': 50, 'n_neighbors': 20, 'p': 1, 'weights': 'distance'}\n",
      "0.946 (+/-0.012) for {'leaf_size': 50, 'n_neighbors': 20, 'p': 2, 'weights': 'uniform'}\n",
      "0.941 (+/-0.016) for {'leaf_size': 50, 'n_neighbors': 20, 'p': 2, 'weights': 'distance'}\n",
      "\n",
      "Detailed classification report:\n",
      "\n",
      "The model is trained on the full development set.\n",
      "The scores are computed on the full evaluation set.\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.95      0.99      0.97       599\n",
      "           2       1.00      0.08      0.15        25\n",
      "           3       0.98      0.98      0.98       162\n",
      "           4       1.00      1.00      1.00        10\n",
      "           5       0.99      0.97      0.98        72\n",
      "           6       1.00      1.00      1.00         7\n",
      "           7       1.00      0.84      0.91        19\n",
      "           8       1.00      1.00      1.00        77\n",
      "\n",
      "    accuracy                           0.96       971\n",
      "   macro avg       0.99      0.86      0.87       971\n",
      "weighted avg       0.96      0.96      0.95       971\n",
      "\n",
      "Confusion Matrix\n",
      "[[594   0   4   0   1   0   0   0]\n",
      " [ 23   2   0   0   0   0   0   0]\n",
      " [  3   0 159   0   0   0   0   0]\n",
      " [  0   0   0  10   0   0   0   0]\n",
      " [  2   0   0   0  70   0   0   0]\n",
      " [  0   0   0   0   0   7   0   0]\n",
      " [  3   0   0   0   0   0  16   0]\n",
      " [  0   0   0   0   0   0   0  77]]\n",
      "F1 Score\n",
      "0.9530975951973104\n",
      "Accuracy Score\n",
      "0.9629248197734295\n",
      "\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, Y_train, Y_test = train_test_split(X_scaled, Y, train_size=0.05,test_size=0.01, random_state=0)\n",
    "\n",
    "# Set the parameters by cross-validation\n",
    "tuned_parameters = [{'weights': ['uniform', 'distance'], 'p': [1, 2]\n",
    "                     ,'n_neighbors':[5,10,15,20],'leaf_size':[20,30,50]}]\n",
    "\n",
    "scores = ['accuracy']\n",
    "\n",
    "for score in scores:\n",
    "    print(\"# Tuning hyper-parameters for %s\" % score)\n",
    "    print()\n",
    "\n",
    "    clf = GridSearchCV(neighbors.KNeighborsClassifier(algorithm='kd_tree'), tuned_parameters, cv=10,\n",
    "                       scoring=score)\n",
    "    clf.fit(X_train, Y_train)\n",
    "\n",
    "    print(\"Best parameters set found on development set:\")\n",
    "    print()\n",
    "    print(clf.best_params_)\n",
    "    print()\n",
    "    print(\"Grid scores on development set:\")\n",
    "    print()\n",
    "    means = clf.cv_results_['mean_test_score']\n",
    "    stds = clf.cv_results_['std_test_score']\n",
    "    for mean, std, params in zip(means, stds, clf.cv_results_['params']):\n",
    "        print(\"%0.3f (+/-%0.03f) for %r\"\n",
    "              % (mean, std * 2, params))\n",
    "    print()\n",
    "\n",
    "    print(\"Detailed classification report:\")\n",
    "    print()\n",
    "    print(\"The model is trained on the full development set.\")\n",
    "    print(\"The scores are computed on the full evaluation set.\")\n",
    "    print()\n",
    "    Y_true, Y_pred = Y_test, clf.predict(X_test)\n",
    "    print(metrics.classification_report(Y_true, Y_pred))\n",
    "    print(\"Confusion Matrix\")\n",
    "    print(metrics.confusion_matrix(Y_true, Y_pred))\n",
    "    print(\"F1 Score\")\n",
    "    print(metrics.f1_score(Y_test, Y_pred, average='weighted'))\n",
    "    print(\"Accuracy Score\")\n",
    "    print(metrics.accuracy_score(Y_true, Y_pred, normalize=True))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Tuning hyper-parameters for accuracy\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\raphael\\pycharmprojects\\forecast_tests\\venv\\lib\\site-packages\\sklearn\\model_selection\\_search.py:813: DeprecationWarning: The default of the `iid` parameter will change from True to False in version 0.22 and will be removed in 0.24. This will change numeric results when test-set sizes are unequal.\n",
      "  DeprecationWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters set found on development set:\n",
      "\n",
      "{'max_depth': 10, 'max_features': 5, 'n_estimators': 500}\n",
      "\n",
      "Grid scores on development set:\n",
      "\n",
      "0.850 (+/-0.137) for {'max_depth': 3, 'max_features': 5, 'n_estimators': 10}\n",
      "0.802 (+/-0.101) for {'max_depth': 3, 'max_features': 5, 'n_estimators': 50}\n",
      "0.805 (+/-0.109) for {'max_depth': 3, 'max_features': 5, 'n_estimators': 100}\n",
      "0.783 (+/-0.034) for {'max_depth': 3, 'max_features': 5, 'n_estimators': 500}\n",
      "0.880 (+/-0.140) for {'max_depth': 3, 'max_features': 10, 'n_estimators': 10}\n",
      "0.911 (+/-0.092) for {'max_depth': 3, 'max_features': 10, 'n_estimators': 50}\n",
      "0.926 (+/-0.016) for {'max_depth': 3, 'max_features': 10, 'n_estimators': 100}\n",
      "0.926 (+/-0.016) for {'max_depth': 3, 'max_features': 10, 'n_estimators': 500}\n",
      "0.899 (+/-0.083) for {'max_depth': 3, 'max_features': 15, 'n_estimators': 10}\n",
      "0.926 (+/-0.016) for {'max_depth': 3, 'max_features': 15, 'n_estimators': 50}\n",
      "0.926 (+/-0.016) for {'max_depth': 3, 'max_features': 15, 'n_estimators': 100}\n",
      "0.926 (+/-0.016) for {'max_depth': 3, 'max_features': 15, 'n_estimators': 500}\n",
      "0.938 (+/-0.016) for {'max_depth': 5, 'max_features': 5, 'n_estimators': 10}\n",
      "0.934 (+/-0.023) for {'max_depth': 5, 'max_features': 5, 'n_estimators': 50}\n",
      "0.935 (+/-0.025) for {'max_depth': 5, 'max_features': 5, 'n_estimators': 100}\n",
      "0.940 (+/-0.018) for {'max_depth': 5, 'max_features': 5, 'n_estimators': 500}\n",
      "0.939 (+/-0.017) for {'max_depth': 5, 'max_features': 10, 'n_estimators': 10}\n",
      "0.939 (+/-0.020) for {'max_depth': 5, 'max_features': 10, 'n_estimators': 50}\n",
      "0.941 (+/-0.022) for {'max_depth': 5, 'max_features': 10, 'n_estimators': 100}\n",
      "0.943 (+/-0.019) for {'max_depth': 5, 'max_features': 10, 'n_estimators': 500}\n",
      "0.939 (+/-0.022) for {'max_depth': 5, 'max_features': 15, 'n_estimators': 10}\n",
      "0.941 (+/-0.015) for {'max_depth': 5, 'max_features': 15, 'n_estimators': 50}\n",
      "0.940 (+/-0.020) for {'max_depth': 5, 'max_features': 15, 'n_estimators': 100}\n",
      "0.941 (+/-0.017) for {'max_depth': 5, 'max_features': 15, 'n_estimators': 500}\n",
      "0.960 (+/-0.018) for {'max_depth': 10, 'max_features': 5, 'n_estimators': 10}\n",
      "0.959 (+/-0.020) for {'max_depth': 10, 'max_features': 5, 'n_estimators': 50}\n",
      "0.960 (+/-0.018) for {'max_depth': 10, 'max_features': 5, 'n_estimators': 100}\n",
      "0.961 (+/-0.018) for {'max_depth': 10, 'max_features': 5, 'n_estimators': 500}\n",
      "0.958 (+/-0.017) for {'max_depth': 10, 'max_features': 10, 'n_estimators': 10}\n",
      "0.959 (+/-0.018) for {'max_depth': 10, 'max_features': 10, 'n_estimators': 50}\n",
      "0.960 (+/-0.020) for {'max_depth': 10, 'max_features': 10, 'n_estimators': 100}\n",
      "0.959 (+/-0.018) for {'max_depth': 10, 'max_features': 10, 'n_estimators': 500}\n",
      "0.958 (+/-0.019) for {'max_depth': 10, 'max_features': 15, 'n_estimators': 10}\n",
      "0.958 (+/-0.018) for {'max_depth': 10, 'max_features': 15, 'n_estimators': 50}\n",
      "0.958 (+/-0.019) for {'max_depth': 10, 'max_features': 15, 'n_estimators': 100}\n",
      "0.959 (+/-0.018) for {'max_depth': 10, 'max_features': 15, 'n_estimators': 500}\n",
      "0.952 (+/-0.018) for {'max_depth': 20, 'max_features': 5, 'n_estimators': 10}\n",
      "0.950 (+/-0.015) for {'max_depth': 20, 'max_features': 5, 'n_estimators': 50}\n",
      "0.950 (+/-0.015) for {'max_depth': 20, 'max_features': 5, 'n_estimators': 100}\n",
      "0.950 (+/-0.016) for {'max_depth': 20, 'max_features': 5, 'n_estimators': 500}\n",
      "0.951 (+/-0.017) for {'max_depth': 20, 'max_features': 10, 'n_estimators': 10}\n",
      "0.950 (+/-0.016) for {'max_depth': 20, 'max_features': 10, 'n_estimators': 50}\n",
      "0.951 (+/-0.017) for {'max_depth': 20, 'max_features': 10, 'n_estimators': 100}\n",
      "0.951 (+/-0.015) for {'max_depth': 20, 'max_features': 10, 'n_estimators': 500}\n",
      "0.951 (+/-0.015) for {'max_depth': 20, 'max_features': 15, 'n_estimators': 10}\n",
      "0.950 (+/-0.016) for {'max_depth': 20, 'max_features': 15, 'n_estimators': 50}\n",
      "0.951 (+/-0.016) for {'max_depth': 20, 'max_features': 15, 'n_estimators': 100}\n",
      "0.951 (+/-0.016) for {'max_depth': 20, 'max_features': 15, 'n_estimators': 500}\n",
      "0.940 (+/-0.022) for {'max_depth': 5, 'max_features': 5, 'n_estimators': 10}\n",
      "0.938 (+/-0.017) for {'max_depth': 5, 'max_features': 5, 'n_estimators': 50}\n",
      "0.939 (+/-0.015) for {'max_depth': 5, 'max_features': 5, 'n_estimators': 100}\n",
      "0.939 (+/-0.017) for {'max_depth': 5, 'max_features': 5, 'n_estimators': 500}\n",
      "0.940 (+/-0.021) for {'max_depth': 5, 'max_features': 10, 'n_estimators': 10}\n",
      "0.940 (+/-0.013) for {'max_depth': 5, 'max_features': 10, 'n_estimators': 50}\n",
      "0.940 (+/-0.018) for {'max_depth': 5, 'max_features': 10, 'n_estimators': 100}\n",
      "0.942 (+/-0.020) for {'max_depth': 5, 'max_features': 10, 'n_estimators': 500}\n",
      "0.941 (+/-0.017) for {'max_depth': 5, 'max_features': 15, 'n_estimators': 10}\n",
      "0.939 (+/-0.019) for {'max_depth': 5, 'max_features': 15, 'n_estimators': 50}\n",
      "0.939 (+/-0.018) for {'max_depth': 5, 'max_features': 15, 'n_estimators': 100}\n",
      "0.940 (+/-0.018) for {'max_depth': 5, 'max_features': 15, 'n_estimators': 500}\n",
      "\n",
      "Detailed classification report:\n",
      "\n",
      "The model is trained on the full development set.\n",
      "The scores are computed on the full evaluation set.\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.97      0.99      0.98       599\n",
      "           2       1.00      0.40      0.57        25\n",
      "           3       0.98      0.98      0.98       162\n",
      "           4       1.00      1.00      1.00        10\n",
      "           5       0.99      0.97      0.98        72\n",
      "           6       1.00      1.00      1.00         7\n",
      "           7       1.00      0.95      0.97        19\n",
      "           8       1.00      1.00      1.00        77\n",
      "\n",
      "    accuracy                           0.97       971\n",
      "   macro avg       0.99      0.91      0.94       971\n",
      "weighted avg       0.97      0.97      0.97       971\n",
      "\n",
      "Confusion Matrix\n",
      "[[594   0   4   0   1   0   0   0]\n",
      " [ 15  10   0   0   0   0   0   0]\n",
      " [  3   0 159   0   0   0   0   0]\n",
      " [  0   0   0  10   0   0   0   0]\n",
      " [  2   0   0   0  70   0   0   0]\n",
      " [  0   0   0   0   0   7   0   0]\n",
      " [  1   0   0   0   0   0  18   0]\n",
      " [  0   0   0   0   0   0   0  77]]\n",
      "F1 Score\n",
      "0.9700760194893012\n",
      "Accuracy Score\n",
      "0.9732234809474768\n",
      "\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, Y_train, Y_test = train_test_split(X_scaled, Y, train_size=0.05,test_size=0.01, random_state=0)\n",
    "\n",
    "# Set the parameters by cross-validation\n",
    "tuned_parameters = [{'n_estimators': [10, 50, 100, 500],'max_depth':[3,5,10,20,5],'max_features':[5,10,15]}]\n",
    "\n",
    "scores = ['accuracy']\n",
    "\n",
    "for score in scores:\n",
    "    print(\"# Tuning hyper-parameters for %s\" % score)\n",
    "    print()\n",
    "\n",
    "    clf = GridSearchCV(ensemble.RandomForestClassifier(criterion='gini'), tuned_parameters, cv=10,\n",
    "                       scoring=score)\n",
    "    clf.fit(X_train, Y_train)\n",
    "\n",
    "    print(\"Best parameters set found on development set:\")\n",
    "    print()\n",
    "    print(clf.best_params_)\n",
    "    print()\n",
    "    print(\"Grid scores on development set:\")\n",
    "    print()\n",
    "    means = clf.cv_results_['mean_test_score']\n",
    "    stds = clf.cv_results_['std_test_score']\n",
    "    for mean, std, params in zip(means, stds, clf.cv_results_['params']):\n",
    "        print(\"%0.3f (+/-%0.03f) for %r\"\n",
    "              % (mean, std * 2, params))\n",
    "    print()\n",
    "\n",
    "    print(\"Detailed classification report:\")\n",
    "    print()\n",
    "    print(\"The model is trained on the full development set.\")\n",
    "    print(\"The scores are computed on the full evaluation set.\")\n",
    "    print()\n",
    "    Y_true, Y_pred = Y_test, clf.predict(X_test)\n",
    "    print(metrics.classification_report(Y_true, Y_pred))\n",
    "    print(\"Confusion Matrix\")\n",
    "    print(metrics.confusion_matrix(Y_true, Y_pred))\n",
    "    print(\"F1 Score\")\n",
    "    print(metrics.f1_score(Y_test, Y_pred, average='weighted'))\n",
    "    print(\"Accuracy Score\")\n",
    "    print(metrics.accuracy_score(Y_true, Y_pred, normalize=True))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Spectral embedding of the digits dataset\n",
    "print(\"Computing Spectral embedding\")\n",
    "embedder = manifold.SpectralEmbedding(n_components=4, random_state=0,\n",
    "                                      eigen_solver=\"arpack\")\n",
    "t0 = time()\n",
    "X_se = embedder.fit_transform(X_scaled)\n",
    "print(\"Spectral embedding of the digits (time %.2fs)\" %(time() - t0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# HLLE embedding of the digits dataset\n",
    "print(\"Computing Hessian LLE embedding\")\n",
    "n_neighbors = 15\n",
    "clf = manifold.LocallyLinearEmbedding(n_neighbors = 15, n_components=4,\n",
    "                                      method='hessian')\n",
    "t0 = time()\n",
    "X_hlle = clf.fit_transform(X_scaled)\n",
    "print(\"Done. Reconstruction error: %g\" % clf.reconstruction_error_)\n",
    "print(\"Hessian Locally Linear Embedding of the digits (time %.2fs)\" %(time() - t0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LTSA embedding of the digits dataset\n",
    "print(\"Computing LTSA embedding\")\n",
    "n_neighbors = 15\n",
    "clf = manifold.LocallyLinearEmbedding(n_neighbors = 15, n_components=4,\n",
    "                                      method='ltsa')\n",
    "t0 = time()\n",
    "X_ltsa = clf.fit_transform(X_scaled)\n",
    "print(\"Done. Reconstruction error: %g\" % clf.reconstruction_error_)\n",
    "print(\"Local Tangent Space Alignment of the digits (time %.2fs)\" %(time() - t0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
